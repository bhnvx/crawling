{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BOX OFFICE 연간 Weekly 데이터 추출\n",
    "\n",
    "set_year = input(\"연도 입력 : \")\n",
    "now = datetime.datetime.now()\n",
    "\n",
    "URL = 'https://www.boxofficemojo.com/weekly/by-year/'+set_year+'/'\n",
    "\n",
    "headers = { \n",
    "'User-Agent' : ('Mozilla/5.0 (Windows NT 10.0;Win64; x64)\\\n",
    "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98\\\n",
    "Safari/537.36')}\n",
    "\n",
    "resq = requests.get(URL, headers=headers)\n",
    "\n",
    "fin_data = []\n",
    "\n",
    "Status = True\n",
    "while Status:\n",
    "    if resq.status_code != 200:\n",
    "        time.sleep(2)\n",
    "        resq = requests.get(URL, headers=headers)\n",
    "    else:\n",
    "        Status = False\n",
    "    \n",
    "soup = BeautifulSoup(resq.text, 'html.parser')\n",
    "\n",
    "data = soup.find_all('td', {'class':True})\n",
    "\n",
    "result = [data[i * 12:(i+1)*12] for i in range((len(data)+11)//12)]\n",
    "\n",
    "for i in tqdm(range(len(result))):\n",
    "    Top10Gross = result[i][1].get_text()\n",
    "    LW1 = result[i][2].get_text()\n",
    "    OverallGross = result[i][3].get_text()\n",
    "    LW2 = result[i][4].get_text()\n",
    "    Release1 = result[i][5].get_text()\n",
    "    Release2 = result[i][6].get_text()\n",
    "    Week = result[i][10].get_text()\n",
    "    reg_date = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    week_list = {'week':Week, 'year':set_year, 'top_gross':Top10Gross, 'top_lw':LW1, 'overall_gross':OverallGross, 'overall_lw':LW2,\n",
    "                'releases':Release1, '#1 Release':Release2, 'top_tmdb_id':None, 'regist_date':reg_date}\n",
    "\n",
    "    fin_data.append(week_list)\n",
    "        \n",
    "df = pd.DataFrame(fin_data, columns=['week', 'year', 'top_gross', 'top_lw', 'overall_gross',  'overall_lw', 'releases', '#1 Release','top_tmdb_id', 'regist_date'])\n",
    "# df.drop_duplicates('#1 Release')\n",
    "\n",
    "filename = set_year + '년 데이터_' + 'boxoffice_result_%s-%s-%s  %s시 %s분 %s초.xlsx' %(now.year, now.month, now.day, now.hour, now.minute, now.second)\n",
    "\n",
    "url_preserve = pd.ExcelWriter(filename, options={'strings_to_urls': False})\n",
    "df.to_excel(url_preserve)\n",
    "url_preserve.save()\n",
    "print('Success')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 각 주차마다 세부 데이터 추출\n",
    "\n",
    "link_data = soup.find_all('a', {'class':'a-link-normal'})\n",
    "clean_data = []\n",
    "\n",
    "for x in range(0, len(link_data)):\n",
    "    if \"weekly/20\" in link_data[x]['href']:\n",
    "        clean_data.append(link_data[x]['href'])\n",
    "        \n",
    "set_list = set(clean_data)\n",
    "clean = list(set_list)\n",
    "\n",
    "d_fin = []\n",
    "\n",
    "for i in tqdm(range(0, len(clean))):\n",
    "        \n",
    "    d_URL = 'https://www.boxofficemojo.com' + clean[i]\n",
    "    \n",
    "    d_resq = requests.get(d_URL, headers=headers)\n",
    "    \n",
    "    Status = True\n",
    "    while Status:\n",
    "        if (d_resq.status_code != 200):\n",
    "            time.sleep(2)\n",
    "            d_resq = requests.get(d_URL, headers=headers)\n",
    "        else:\n",
    "            Status = False\n",
    "    \n",
    "    d_soup = BeautifulSoup(d_resq.text, 'html.parser')\n",
    "    \n",
    "    d_data = d_soup.find_all('td', {'class' : True})\n",
    "    \n",
    "    d_result = [d_data[i * 13:(i+1)*13] for i in range((len(d_data)+12)//13)]\n",
    "    \n",
    "    d_week = len(clean)-i\n",
    "    \n",
    "    for j in range(0, len(d_result)):\n",
    "        d_title = d_result[j][2].get_text()\n",
    "        d_url = 'https://www.boxofficemojo.com' + clean[i]\n",
    "        d_gross = d_result[j][3].get_text()\n",
    "        d_average = d_result[j][7].get_text()\n",
    "        d_total_gross = d_result[j][8].get_text()\n",
    "        reg_date = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        # imdb_id 구하기\n",
    "        d_r_req = requests.get('https://www.boxofficemojo.com' + d_result[j][2].find('a')['href'], headers=headers)\n",
    "        Status = True\n",
    "        while Status:\n",
    "            if (d_r_req.status_code != 200):\n",
    "                time.sleep(2)\n",
    "                d_r_req = requests.get('https://www.boxofficemojo.com' + d_result[j][2].find('a')['href'], headers=headers)\n",
    "            else:\n",
    "                Status = False\n",
    "        d_r_soup = BeautifulSoup(d_r_req.text, 'html.parser')\n",
    "        \n",
    "        if d_r_soup.find_all('div', {'id':'title-summary-refiner'}) != None:\n",
    "            imdb_id = ''.join(re.findall('e\\/(.+?)\\/\\?', str(d_r_soup.find('div', {'id':'title-summary-refiner'}))))\n",
    "        else:\n",
    "            imdb_id = None\n",
    "        \n",
    "        d_result_data = {'tmdb_id':None, 'imdb_id':imdb_id, 'title':d_title,\n",
    "                    'url':d_url, 'series_id':None, 'week':d_week, 'year':set_year, 'gross':d_gross,\n",
    "                    'average':d_average, 'total_gross':d_total_gross, 'regist_date':reg_date}\n",
    "        d_fin.append(d_result_data)\n",
    "    \n",
    "d_df = pd.DataFrame(d_fin, columns=['tmdb_id', 'imdb_id', 'title', 'url', 'series_id',  'week', 'year', 'gross','average','total_gross', 'regist_date'])\n",
    "\n",
    "d_filename = set_year + '년 데이터_' + 'boxoffice_detail_result_%s-%s-%s  %s시 %s분 %s초.xlsx' %(now.year, now.month, now.day, now.hour, now.minute, now.second)\n",
    "\n",
    "d_url_preserve = pd.ExcelWriter(d_filename, options={'strings_to_urls': False})\n",
    "d_df.to_excel(d_url_preserve)\n",
    "d_url_preserve.save()\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
